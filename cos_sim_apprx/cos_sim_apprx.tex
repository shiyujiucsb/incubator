\documentclass{article}

\usepackage{graphicx,tikz}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage{booktabs}
%\usepackage{color}

%\renewcommand{\baselinestretch}{0.95}
%\setlength{\textfloatsep}{0.1cm}
%\setlength{\abovecaptionskip}{0.1cm}

\begin{document}

\title{\Large\bf Approximating All-Pairs Similarity Search by Rademacher Average}
\author{Shiyu Ji\\ shiyu@cs.ucsb.edu}
\date{}
\maketitle

\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}
\theoremstyle{plain}
\newtheorem{corollary}{Corollary}
\theoremstyle{plain}

\section{Introduction}
All-pairs similarity search (APSS) has received extensive research interest recently \cite{BMS07,Xia16,ATY13,TAJY14}. To improve performance, many approximation approaches have been proposed \cite{GIM99,FKS03,IM98,Char02}. This paper considers two approximation methods for cosine similarity based search \cite{SGM00,Xia16,ATY13,TAJY14}, and SimRank based search \cite{JW02,LH10,FNS13,KMK14} respectively. The approximation error of each our algorithm is upper bounded by using Rademacher average \cite{BM02,Mohri09,BBM05}.

\section{Related Works}
All-pairs similarity search (APSS) is recently a popular research topic in the community of information retrieval \cite{BMS07,Xia16,ATY13,TAJY14}. Given a big set of objects, the goal of APSS is to efficiently compute (or approximate) the similarities between each pair of objects. Many similarity measures have been proposed \cite{SGM00}, e.g., Cosine Similarity \cite{TP07}, SimRank \cite{JW02}, Jaccard Index \cite{HHH89}, Metric Distance \cite{SGM00}, Pearson Correlation \cite{BCY09}, etc. For similarity search, Cosine Similarity and SimRank are very popular (\cite{TP07,Xia16,ATY13,TAJY14} for Cosine, and \cite{LH10,FNS13,KMK14,YM15} for SimRank). A major challenge of APSS is the large volume of computation: given $n$ objects, without any assumption on the similarity distribution (e.g., the similarity matrix is sparse), the time complexity is at least $O(n^2)$. To compute more efficiently, the state-of-art research works often use parallelism \cite{CCK12,HFL10} and dissimilarity detection \cite{ATY13,TAJY14}. If only approximations are needed, how to efficiently sample with negligible error for similarity search is another research focus \cite{GIM99,FKS03,IM98,Char02}. This paper focuses on the approximation problem.

We may treat the similarity approximation as a learning problem over large scaled data, and we need to upper bound the learning error for the worst case. In statistical learning theory, such upper bounds are usually called risk bounds \cite{Vap13}. There are two classical methods to compute the risk bounds: by Vapnikâ€“Chervonenkis (VC) dimension \cite{VLL94,Vap98,Vap13} and by Rademacher average \cite{Mohri09,BM02,BBM05}. Many approximation algorithms that use these two techniques have been proposed \cite{RK14,RK16,RU15,RU16}. Riondato and Kornaropoulos \cite{RK14,RK16} proposed algorithms that use VC dimension to upper bound the sample size that is sufficient to approximate the betweenness centralities \cite{Bran01} of all nodes in a graph with guaranteed error bounds. One limitation of the VC-based algorithms is that the upper bounds of some characteristic quantities (e.g., the maximum length of any shortest path) are needed \cite{RK14,RK16,RU16}. But such bounds are not always available. One year later, Riondato and Upfal used Rademacher average to approximate the frequent itemsets \cite{RU15} and betweenness centralities \cite{RU16}. They also found that by using Rademacher average, we can avoid the aforementioned limitation of VC-based solutions. It has been proved that Rademacher average can be applied to various approximation problems, which are out of the scope of classical learning framework. This paper basically follows this idea. To the best of our knowledge, there is no research work connecting similarity approximation with Rademacher average. We attempt to fill this void.

\section{Problem Formulation and Preliminaries}
\subsection{Cosine Similarity}
We consider the cosine similarity based all-pairs similarity search.
Suppose there are $n$ vectors (each vector can represent a user profile or a web page). Each vector contains $m$ non-negative features. Define the cosine similarity between two vectors $u$ and $v$ as
$$Sim(u,v) = \frac{1}{||u||\cdot||v||} \sum_{i=1}^m u_i\cdot v_i,$$
where $u_i$, $v_i$ denotes the $i$-th feature value of $u$, $v$.
For simplicity, we assume all the vectors are adjusted with the same norm: $||v|| = \sqrt{m}$ for every $v$ in the $n$ vectors. Then the equation above can be simplified as
$$Sim(u,v) = \frac{1}{m} \sum_{i=1}^m u_i\cdot v_i.$$
That is, the similarity is defined as the average on the corresponding feature products between the vectors. 

To do the all-pairs similarity search (APSS), we need to compute the similarity between each pair of vectors. Since there are $n(n-1)$ pairs, and for each pair we need $m$ times of multiplication, the total complexity of a naiive algorithm is $O(n^2 m)$. Fortunately, there are many methods to detect dissimilar pairs (two vectors without sharing any feature) \cite{ATY13,TAJY14,Lin09}, which can save a lot of computation. For the state-of-art works, to compute all the pairs, the complexity can be lowered to $O(nkm)$, where $k$ is much less than $n$. However, to the best of our knowledge, there were few discussions on the size of features $m$. If we can only consider a part of the $m$ features without significantly deteriorating the accuracy, then the total computation time for APSS can be lowered significantly (note that $nk$ is still large for very big dataset). We can approximating the similarity by sampling on the features.

\subsection{SimRank}
SimRank \cite{JW02} is a popular measure of the similarity between two nodes in a graph based on the idea that similar nodes are often referred by other similar nodes. Denote by $s(a,b)$ the SimRank between nodes $a$ and $b$. If $a=b$, then $s(a,b)$ is defined to be 1. Otherwise, 
$$s(a,b) = \frac{c}{|I(a)|\cdot|I(b)|}\sum_{i\in I(a)}\sum_{j\in I(b)}s(i,j),$$
where $c$ is a constant in $(0,1)$ and $I(a)$ denotes the in-neighbors of $a$. If there is an edge from $a$ to $b$, then $a$ is an in-neighbor of $b$. 
An important fact is that SimRank can be equivalently built upon Random Surfer-Pairs Model \cite{JW02}. That is, $s(a,b)$ can also be written as follows:
$$s(a,b) = \sum_{t: (a,b) (x,x)} P[t]\cdot c^{L(t)},$$
where $t$ is a pair of random walks with the same number of steps, which lead $a$ and $b$ to meet at one node $x$, and $P[t]$ denotes the probability when $t$ is chosen, and $L(t)$ denotes the number of steps of each walk in $t$. Here we take random walks from the reversed graph, in which each edge is reversed compared with the original graph. A random walk works as follows: in the tour at any node, which has $k$ out-edges, we take one of the $k$ edge with equal probability $1/k$ as our next step. Note that as the length of random walk grows, its contribution to $s(a,b)$ decreases exponentially. Hence in practice, we only need to consider relatively short walks, e.g., with length no more than $T$.

\subsection{Rademacher Average}
This section proposes our approximation algorithm and its analysis.
Suppose we want to compute the cosine similarities between a fixed vector $u$ and other vectors $v_1$, $v_2$, $\cdots$, $v_n$. We take the $m$ features as the sample space $S$:
$$S = \{s_1, \cdots, s_k\} \subseteq D$$
where $k$ is the number of samples we take, and $D$ is the feature space: $D = \{1,2,\cdots,m\}$.
Let $F$ be a collection of functions from the features $D$ to the interval $[0, m]$.
For each function $f\in F$, define the \emph{true average} $A_D(f)$ and \emph{sampled average} $A_S(f)$ as follows:
$$A_D(f) = \frac{1}{m} \sum_{i=1}^m f(i),\quad A_S(f) = \frac{1}{k} \sum_{i=1}^k f(s_i).$$
Define the \emph{uniform deviation} \cite{Oneto13} of $F$ given $S$ as
$$U_S(F) = \sup_{f\in F} [ A_S(f) - A_D(f) ].$$
Note that if $F$ is a finite set (in this paper we will see this is true), supreme can be replaced by maximum:
$$U_S(F) = \max_{f\in F} [ A_S(f) - A_D(f) ].$$
Define the \emph{Rademacher average} \cite{Mohri09,BM02,Oneto13} of $F$ given $S$ as
\newcommand{\E}{\mathbb{E}}
$$R_S(F) = \E_\sigma \left[\sup_{f\in F} \frac{2}{k}\sum_{i=1}^k \sigma_i f(s_i) \right],$$
where each $\sigma_i$ is a random variable uniformly distributed over $\{-1, 1\}$, and the mean $\E_\sigma$ takes randomness over all the $\sigma_i$'s conditionally on $S$. Again, one can replace supreme by maximum.

The main motivation of our algorithm is that the difference between true average and sampled average is upper bounded by the Rademacher average as follows.
We leave the proofs to the Appendix.

\begin{theorem}
\label{thm:main}
Let $F$ be a collection of functions $f$ mapping $D$ to $[0,m]$. 
With probability at least $1-\delta$, we have
$$\sup_{f\in F}|A_S(f) - A_D(f)| \leq R_S(F) + \left(m+m\sqrt{\frac{8}{k}\log \frac{2}{\delta}} + m\sqrt{\frac{8}{k}\log \frac{2}{\delta} + \frac{8R_S(F)}{m}}\right)\sqrt{\frac{\log \frac{8}{\delta}}{2k}}.$$
\end{theorem}

The remaining item we need to upper bound is $R_S(F)$. Our result is similar to Massart's lemma \cite{AGO14}.

\begin{theorem}
\label{thm2}
$$R_S(F) \leq \frac{\ell}{k}\sqrt{8\log |F|},$$
where $\ell^2 = \sup_{f\in F}\sum_{i=1}^k f(s_i)^2$.
\end{theorem}

By the above two theorems, we can bound our approximation error even for the worst case.


\section{Approximating Cosine Similarities}
The approximation algorithm takes as input a collection of $n$ vectors $V$ and two parameters $(\epsilon, \delta)$ whose values are between 0 and 1. The algorithm outputs a set $C = \{\tilde{S}(u,v): u,v \in V, u\not= v\}$, where $\tilde{S}(u,v)$ is the $(\epsilon, \delta)$- approximation of cosine similarity between $u$ and $v$, i.e., with probability at least $1-\delta$, the worst approximation error in $C$ is at most $\epsilon$. Each vector in $V$ contains $m$ features. We take the $m$ features as the sample space $D = \{1,\cdots,m\}$. For each feature $s\in D$, let $f_{u,v}(s) = u_s\cdot v_s$, where $u_s$ is the $s$-th feature value of the vector $u$. Let $F = \{f_{u,v}: u,v\in V, u\not= v\}$. Thus $|F| < n^2/2$ since symmetry of cosine similarity. It is clear that the true average of $f_{u,v}$ equals to the cosine similarity between $u$ and $v$. Given the sampled features, the upper bound of $R_S(F)$ is 
$$R_S(F) \leq \frac{\ell}{k}\sqrt{8\log |F|} < \frac{4\ell\sqrt{\log n}}{k},$$
where $\ell = \sqrt{\max_{f\in F} \sum_{i=1}^k f(s_i)^2}$. Since $F$ is finite, we can replace supreme by maximum. 

\subsection{The Algorithm}
The approximation algorithm works in an iterative mode. For each iteration, we sample some new features $s_i$'s among $D$ and aggregate the feature products given by $f_{u,v}(s_i)$ for each $f_{u,v}$ in $F$. Then we compute the error upper bound:
$$\Delta = \frac{4\ell\sqrt{\log n}}{k} +\left(m+m\sqrt{\frac{8}{k}\log \frac{2}{\delta}} + m\sqrt{\frac{8}{k}\log \frac{2}{\delta} + \frac{32\ell\sqrt{\log n}}{mk}}\right)\sqrt{\frac{\log \frac{8}{\delta}}{2k}},$$
where $\ell = \sqrt{\max_{f\in F} \sum_{i=1}^k f(s_i)^2}$, and $k$ is the number of aggregated samples. 
If $\Delta \leq \epsilon$, then we stop and return the averages $\frac{1}{k}\sum_{s_i\in S}f_{u,v}(s_i)$ for each pair $u, v$. Otherwise, we continue to the next round, where we will sample more features. If $\Delta \leq \epsilon$ can never be satisfied, at the end we will sample all the $m$ features and return the averages as the exact solution. Algorithm \ref{alg:csa} gives the pseudocode of our solution.

It is clear to verify the correctness of our algorithm by Theorem \ref{thm:main} and Theorem \ref{thm2}.

\begin{algorithm}[!t]
\caption{\textsf{Cosine Similarity Approximation}}
\label{alg:csa}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}
\Require{vectors $V$ ($|V|=n$); features $U$ ($|U|=m$); $\epsilon \in (0,1)$; $\delta \in (0,1)$.}
\Ensure{$Sim(u,v)$ for each $u, v \in V$ s.t. $u\not= v$.}
\State $k \gets 0$;
\State $S \gets \emptyset$;
\State $S(u,v) \gets 0$ for each $u, v \in V$ s.t. $u\not= v$;
\While {$k \leq m$}
	\State $k' \gets \mathsf{next-sample-size}(k)$;
	\For {$i$ from $k$ to $k'-1$}
		\State Uniformly sample a feature $s$ from $U\setminus S$;
		\State $S(u,v) \gets S(u,v)+u[s]\cdot v[s]$;
		\State $S \gets S\cup \{s\}$;
	\EndFor
	\State $\Delta \gets \frac{4\ell\sqrt{\log n}}{k} +\left(m+m\sqrt{\frac{8}{k}\log \frac{2}{\delta}} + m\sqrt{\frac{8}{k}\log \frac{2}{\delta} + \frac{32\ell\sqrt{\log n}}{mk}}\right)\sqrt{\frac{\log \frac{8}{\delta}}{2k}}$;
	\If {$\Delta \leq \epsilon$}
		\State break the while loop;
	\EndIf
\EndWhile
\State $\mathsf{S} \gets \{S(u,v)/k : u,v\in V, u\not=v\}$.
\State {\bf return} $\mathsf{S}$.
\end{algorithmic}
\end{algorithm}

\subsection{Improving the Upper Bound}
\label{sec:imprb}
Note that in the upper bound $\Delta$ of Algorithm \ref{alg:csa}, $m$ is a dominating variable: it is proportional to $\Delta$. Usually $m$ can be quite large, e.g., from 100 to 10K. Thus it is worth investigating how to mitigate the negative effect of large $m$. The idea comes from Theorem \ref{thm:main}, which assumes the range of each function $f$ is $[0,m]$. However in reality, most feature products $u[s]\cdot v[s]$ are close to zero due to sparsity, and its maximum value shall be much less than $m$. In practice, usually we can assume there is a good estimation (much less than $m$) on the maximum feature product based on historic experience. To further lower the upper bound $\Delta$, a variant of Algorithm \ref{alg:csa} may use the bound as follows:
$$\hat{m} = \max_{i\in [1,\cdots,m], u,v \in V, u\not= v}u[i]\cdot v[i],$$
where $[1,\cdots,m]$ denotes all the $m$ features, and $V$ is the set of all vectors. Intuitively, $\hat{m}$ is the maximum value of all the feature products, and we assume we have a good estimation of $\hat{m}$. 
Then we may choose the upper bound as follows:
$$\Delta = \frac{4\ell\sqrt{\log n}}{k} +\hat{m}\left(1+\sqrt{\frac{8}{k}\log \frac{2}{\delta}} + \sqrt{\frac{8}{k}\log \frac{2}{\delta} + \frac{32\ell\sqrt{\log n}}{\hat{m}k}}\right)\sqrt{\frac{\log \frac{8}{\delta}}{2k}}.$$
We use experiments (in Section \ref{sec:eval}) to show that the improvement of the variant above is significant. 

\section{Approximating SimRank}
We approximate SimRank scores on the nodes in the digraph $G=(V,E)$. 
The basic idea is similar to the case of cosine similarity approximation, but the sample space $D$ is changed to the $2T$-dimensional manifold $[0,1]^{2T}$, where $[0,1]$ is the interval of real numbers between 0 and 1, and $F$ is now defined as
$$F = \{f_{a,b} : a,b\in V, a\not= b\},$$
where given any two nodes $a$, $b$, the function $f_{a,b}$ takes a sample $s_i\in D$ as input and outputs a value in the interval $[0,c]$. Algorithm \ref{alg:rwg} shows how to generate a random walk given a sample from $[0,1]^T$ and a starting node $a$. It is clear that the random walk chosen by Algorithm \ref{alg:rwg} is uniformly distributed if the sample is uniformly drawn from $[0,1]^T$. Thus given two nodes $a$, $b$ and a sample $s_i \in [0,1]^{2T}$, we can generate two random walks $W_a$, $W_b$ starting from $a$ and $b$ respectively. Let 
$$l_{a,b}(s_i) = \min\{j: 1\leq j\leq T, W_a[j] = W_b[j]\},$$
i.e., the number of steps before two surfers starting from $a$, $b$ meet at some node. If $W_a$ and $W_b$ never meet within $T$ steps, $l_{a,b}(s_i)$ is defined to be 0.

\begin{algorithm}[!t]
\caption{\textsf{Random Walk Generation}}
\label{alg:rwg}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}
\Require{reversed graph $\overline{G}$, sequence $(s_1,\cdots,s_T) \in [0,1]^T$, starting node $a$.}
\Ensure{a random walk $W = (a,\cdots)$ starting from $a$.}
\State Let $W$ be a sequence of length $T+1$;
\State $W[0] \gets a$;
\For {$i$ from 1 to $T$}
	\State $I \gets \{b: \textrm{there is an edge in $\overline{G}$ from $W[i-1]$ to $b$}\}$;
	\State $k \gets |I|$;
	\If{k=0}
		\State $W[i] \gets W[i-1]$;
	\Else
		\State Sort $I$ in lexicographical order: $(I[0],\cdots,I[k-1])$;
		\State $x \gets \lfloor s_{i-1} * k \rfloor$;
		\State $W[i] \gets I[x]$;
	\EndIf
\EndFor
\State {\bf return} $W$.
\end{algorithmic}
\end{algorithm}

The function $f_{a,b}(s_i)$ is defined as 
$$f_{a,b}(s_i) =\begin{cases}
0 & \textrm{if $l_{a,b}(s_i)=0$,} \\
c^{l_{a,b}(s_i)} & \textrm{otherwise}.\\
\end{cases}$$
It is clear that the true average $A_D(f_{a,b})$ equals to the SimRank between the nodes $a$ and $b$. Thus a similar approximation idea based on Rademacher average can be applied. We still need to upper bound the worst approximation error. The new bound is stated as follows.

\begin{theorem}
\label{thm:new}
Let $F$ be a collection of functions $f$ mapping $D$ to $[0,c]$. 
With probability at least $1-\delta$, we have
$$\sup_{f\in F}|A_S(f) - A_D(f)| \leq R_S(F) + \left(c+c\sqrt{\frac{8}{k}\log \frac{2}{\delta}} + c\sqrt{\frac{8}{k}\log \frac{2}{\delta} + \frac{8R_S(F)}{c}}\right)\sqrt{\frac{\log \frac{8}{\delta}}{2k}}.$$
\end{theorem}
The proof is essentially the same as the proof of Theorem \ref{thm:main}. We leave the discussions to the Appendix. Also the upper bound of $R_S(F)$ given by Theorem \ref{thm2} remains unchanged.

Our approximation algorithm works as follows. For each iteration, we sample some new features $s_i$'s among $D$, generate random walks $W_a$, $W_b$ from nodes $a$, $b$ given $s_i$ and aggregate $f_{a,b}(s_i)$ for each $f_{a,b}$ in $F$. Then we compute the error upper bound:
$$\Delta = \frac{4\ell\sqrt{\log n}}{k} +\left(c+c\sqrt{\frac{8}{k}\log \frac{2}{\delta}} + c\sqrt{\frac{8}{k}\log \frac{2}{\delta} + \frac{32\ell\sqrt{\log n}}{ck}}\right)\sqrt{\frac{\log \frac{8}{\delta}}{2k}},$$
where $\ell = \sqrt{\max_{f\in F} \sum_{i=1}^k f(s_i)^2}$, and $k$ is the aggregated samples. 
If $\Delta \leq \epsilon$, then we stop and return the averages $\frac{1}{k}\sum_{s_i\in S}f_{a,b}(s_i)$ for each pair $u, v$. Otherwise, we continue to the next round, where we will sample more features. If $\Delta \leq \epsilon$ can never be satisfied with a certain number of iterations, we return the approximated SimRank scores of node pairs, with the message that the input parameters $(\epsilon,\delta)$ cannot be satisfied by our algorithm.

It is clear to verify the correctness of our algorithm by Theorem \ref{thm:new} and Theorem \ref{thm2}.

\begin{algorithm}[!t]
\caption{\textsf{SimRank Approximation}}
\label{alg:sra}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}
\Require{reversed graph $\overline{G}$; $\epsilon \in (0,1)$; $\delta \in (0,1)$; maximum random walk length $T$; maximum number of iterations $R$; SimRank constant $c$.}
\Ensure{$\mathsf{SimRank}(u,v)$ for each $u, v \in V$ s.t. $u\not= v$.}
\State $k \gets 0$;
\State $S(u,v) \gets 0$ for each $u, v \in V$ s.t. $u\not= v$;
\For {$\mathsf{count}$ from 1 to $R$}
	\State $k' \gets \mathsf{next-sample-size}(k)$;
	\For {$i$ from $k$ to $k'-1$}
		\State Uniformly sample a feature $s$ from $[0,1]^{2T}$;
		\For {each $a,b \in V\times V$ s.t. $a\not= b$}
			\State $W_a \gets \textsf{Random Walk Generation}(s[1\cdots T])$;
			\State $W_b \gets \textsf{Random Walk Generation}(s[(T+1)\cdots 2T])$;
			\State $L \gets \{j: 1\leq j\leq T, W_a[j] = W_b[j]\}$;
			\If {$|L| > 0$}
				\State $l_{a,b} \gets \min L$;
				\State $S(u,v) \gets S(u,v)+c^{l_{a,b}}$;
			\EndIf
		\EndFor
	\EndFor
	\State $\Delta \gets \frac{4\ell\sqrt{\log n}}{k} +\left(c+c\sqrt{\frac{8}{k}\log \frac{2}{\delta}} + c\sqrt{\frac{8}{k}\log \frac{2}{\delta} + \frac{32\ell\sqrt{\log n}}{ck}}\right)\sqrt{\frac{\log \frac{8}{\delta}}{2k}}$;
	\If {$\Delta \leq \epsilon$}
		\State break the while loop;
	\EndIf
\EndFor
\State $\mathsf{S} \gets \{S(u,v)/k : u,v\in V, u\not=v\}$.
\If {$\mathsf{count} = R$}
	\State show a message that $(\epsilon,\delta)$ cannot be satisfied.
\EndIf
\State {\bf return} $\mathsf{S}$.
\end{algorithmic}
\end{algorithm}

\section{Evaluation}
\label{sec:eval}
In this section we evaluate our algorithms.

\subsection{Setup}
We implemented our algorithms by Python 3.4.3 and ran the programs on a Fedora 20 (Linux) cluster. Every measurement was averaged over 100 test cases.

For the cosine similarity approximation, we have 100 vectors with 1000 features. We considered two distributions of the feature values: uniform distribution and normal distribution. For uniform distribution, each feature value uniformly distributes over the interval $[0,1]$. For normal distribution, each feature value is generated by $|X|$, where $X$ is a random variable of standard normal distribution $\mathcal{N}(0,1)$. At last, each vector was normalized with norm $\sqrt{m}$. We set the error probability $\delta = 0.0001$.

For the SimRank approximation, we have 100 nodes in total. We implemented two graph models: random graph model \cite{Gil59} and small-world model \cite{WS98}. In random graph model, each pair of nodes are connected by an edge with probability $p=0.1$. In small-world model, each node is connected by an edge with its 4 nearest neighbors, and each other pair of nodes are connected by an edge with probability $p=0.1$. We choose the constant $c=0.7$. We set the error probability $\delta = 0.0001$.

\subsection{Approximation Errors and Sample Sizes}

\begin{figure}[!t]
\centering
\begin{minipage}{.45\textwidth}
\centering
\includegraphics[width=.9\textwidth]{cos_normal_apprx_error.eps}
\caption{\textsf{MaxError}s and upper bounds for cosine similarity with normal distribution.}
\label{fig:cnae}
\end{minipage}
\hspace{1cm}
\begin{minipage}{.45\textwidth}
\centering
\includegraphics[width=.9\textwidth]{cos_uniform_apprx_error.eps}
\caption{\textsf{MaxError} and upper bounds for cosine similarity with uniform distribution.}
\label{fig:cuae}
\end{minipage}
\end{figure}

Here \textsf{MaxError} denotes the maximum absolute error among all the vector pairs, i.e.,
$$\textsf{MaxError} = \max_{u,v\in V,u\not= v} |\widetilde{Sim}(u,v) - Sim(u,v)|,$$
where $\widetilde{Sim}(u,v)$ is the approximated similarity between $u$ and $v$ given by our algorithms. The upper bound \textsf{Delta} is the $\Delta$ given in Algorithm \ref{alg:csa}, and the improved bound is given in Section \ref{sec:imprb}.
For both figures, when more samples are drawn, the maximum absolute errors and upper bounds decrease. Also our improved upper bounds are significantly tighter than the bounds by $m$, but they are still much larger than the maximum absolute errors. Hence our theoretical bounds are still quite loose. However, our improved bounds can be sufficient for practical use. For uniform distribution, by 200 samplings, we can guarantee that the maximum absolute error among all the vector pairs is less than 9.0 with probability at least 99.99\%. Note that the cosine similarities can be between 0 and $m=1000$. Thus 9.0 is relatively small. Similarly for normal distribution, by 190 samplings we can guarantee that with probability at least 99.99\%, the maximum absolute error is less than 10.0.

\begin{figure}[!t]
\centering
\begin{minipage}{.45\textwidth}
\centering
\includegraphics[width=.9\textwidth]{simrank_rg_apprx_error.eps}
\caption{\textsf{MaxError} and upper bounds for SimRank with random graph model.}
\label{fig:rgae}
\end{minipage}
\hspace{1cm}
\begin{minipage}{.45\textwidth}
\centering
\includegraphics[width=.9\textwidth]{simrank_sw_apprx_error.eps}
\caption{\textsf{MaxError} and upper bounds for SimRank with small-world model.}
\label{fig:swae}
\end{minipage}
\end{figure}

Here the upper bound \textsf{Delta} is given in Algorithm \ref{alg:sra}. \textsf{MaxError} is same as before. 
For both figures, when more samples are drawn, the maximum absolute errors and upper bounds decrease. Note that even though our bounds are still quite loose compared to the maximum absolute errors, they can be practically used. For random graph model, by taking 10000 samples, we can guarantee that the maximum absolute error among all the node pairs is less than 0.062 with probability at least 99.99\%. For small-world model, by taking 

%\bibliographystyle{./IEEEtran}
\bibliographystyle{plain}
\bibliography{./cos}

\section{Appendix}
\input{appendix.tex}
\end{document}